{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fe84f433a9714129a15ffb6d2166396a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_89a5e6e8674b43e188156e6f0f4d3466",
              "IPY_MODEL_9eeed2ed02e2495fac33b80a02fd40e0",
              "IPY_MODEL_785ff94615f9476794ba0d192cb0d6b2"
            ],
            "layout": "IPY_MODEL_0523c003089b4757a5a5ec983b788c4a"
          }
        },
        "89a5e6e8674b43e188156e6f0f4d3466": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6a81a1fcd964f0d82f89bc145e40ddf",
            "placeholder": "​",
            "style": "IPY_MODEL_6fdb5dd4ea194ea9959fa73ccbeb0fe4",
            "value": "Parse safetensors files: 100%"
          }
        },
        "9eeed2ed02e2495fac33b80a02fd40e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f7710e1e5ca4832bcb590bd4079703d",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_df81d73394784b3c8e5f56d3657e9512",
            "value": 2
          }
        },
        "785ff94615f9476794ba0d192cb0d6b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8a386585d764e2dbe536a4d2808616c",
            "placeholder": "​",
            "style": "IPY_MODEL_fc85c0288e0b4e15bccd192772eb5870",
            "value": " 2/2 [00:00&lt;00:00,  2.37it/s]"
          }
        },
        "0523c003089b4757a5a5ec983b788c4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6a81a1fcd964f0d82f89bc145e40ddf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fdb5dd4ea194ea9959fa73ccbeb0fe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f7710e1e5ca4832bcb590bd4079703d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df81d73394784b3c8e5f56d3657e9512": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c8a386585d764e2dbe536a4d2808616c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc85c0288e0b4e15bccd192772eb5870": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a37b1ae794945b488f7aba65ec65b55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e0859a00ac5946bfbf30227aaf22c66a",
              "IPY_MODEL_7a70128d94554bb9a98da308dcdcc708",
              "IPY_MODEL_548b0822e46b4c79b82bdd509ece656e"
            ],
            "layout": "IPY_MODEL_370fb5ce556a48a29dab80411f1a2a4e"
          }
        },
        "e0859a00ac5946bfbf30227aaf22c66a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96d2ebfe8d4c4e80968740080d025c29",
            "placeholder": "​",
            "style": "IPY_MODEL_c1325fb7172241e0bd3984368a9137cc",
            "value": ""
          }
        },
        "7a70128d94554bb9a98da308dcdcc708": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5730559795f74ec893c2ae72fb945e2d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_09bacea4ecb244a7b15928e6efd32b85",
            "value": 1
          }
        },
        "548b0822e46b4c79b82bdd509ece656e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22a8210d44054130bd040481a0623b3e",
            "placeholder": "​",
            "style": "IPY_MODEL_009d99e75dde4466889dd5f33cf4f704",
            "value": " 50/? [08:58&lt;00:00, 10.12s/it]"
          }
        },
        "370fb5ce556a48a29dab80411f1a2a4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96d2ebfe8d4c4e80968740080d025c29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1325fb7172241e0bd3984368a9137cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5730559795f74ec893c2ae72fb945e2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "09bacea4ecb244a7b15928e6efd32b85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "22a8210d44054130bd040481a0623b3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "009d99e75dde4466889dd5f33cf4f704": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabi1771/data_science_projects/blob/main/RAG_trainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<h1>\n",
        "<h1>APM 53674: ALTeGraD</h1>\n",
        "<h2>Lab Session 4: Distillation and Retrieval Augmented Generation</h2>\n",
        "<h4>Lecture: Dr. Guokan Shang<br>\n",
        "Lab: Yang Zhang and Xiao Fei</h4>\n",
        "<h5>Tuesday, November 18, 2025</h5>\n",
        "<br>\n",
        "</center>\n",
        "\n",
        "<hr style=\"border:10px solid gray\"> </hr>\n",
        "<p style=\"text-align: justify;\">\n",
        "This handout includes theoretical introductions, <font color='blue'>coding tasks</font> and <font color='red'>questions</font>. Before the deadline, you should submit <a href='https://forms.gle/9dyaes6dimfvyjwq6' target=\"_blank\">here</a> a <B>.ipynb</B> file named <b>Lastname_Firstname.ipynb</b> containing your notebook (with the gaps filled and your answers to the questions). Your answers should be well constructed and well justified. They should not repeat the question or generalities in the handout. When relevant, you are welcome to include figures, equations and tables derived from your own computations, theoretical proofs or qualitative explanations. One submission is required for each student. The deadline for this lab is <b>Novemver 23\n",
        ", 2025 11:59 PM</b>. No extension will be granted. Late policy is as follows: ]0, 24] hours late → -5 pts; ]24, 48] hours late → -10 pts; > 48 hours late → not graded (zero).\n",
        "</p>\n",
        "<hr style=\"border:5px solid gray\"> </hr>"
      ],
      "metadata": {
        "id": "fk4xRt_adQ0Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Requirements"
      ],
      "metadata": {
        "id": "SMA5hf82PxnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required dependencies\n",
        "!pip -q install torch tqdm jsonlines h5py\n",
        "!pip -q install --upgrade transformers accelerate vllm\n",
        "!pip install jedi\n",
        "# !pip -q install datasets==2.21.0 pandas==2.2.2\n",
        "# !pip -q install chromadb==0.4.22\n",
        "# !pip -q install \"numpy<2.0\" --force-reinstall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NTeIal8P0Tb",
        "outputId": "749c2757-500a-4091-8b40-da9418773ebf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jedi in /usr/local/lib/python3.12/dist-packages (0.19.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi) (0.8.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1 - Model Distillation\n",
        "\n",
        "> In this section, you’ll learn the difference between **white-box** and **black-box** distillation, generate **synthetic data** to train a **student model**, and implement **white-box distillation** to specialize a model for a **RAG on Wikipedia** use case.\n"
      ],
      "metadata": {
        "id": "QRiJxB6PdljI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='red'>\n",
        "\n",
        "<hr style=\"border:10px solid red\"> </hr>  \n",
        "Question 1:</br>\n",
        "Explain briefly the difference between black-box and white-box distillation? </br> What are the advantages and inconvenients of each approach?\n",
        "<hr style=\"border:10px solid red\"> </hr>  \n",
        "</font></h4>"
      ],
      "metadata": {
        "id": "07oSzYm_RsK9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 1: </b><br>\n",
        "Your answer here.\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>\n",
        "\n",
        "### **Black-box distillation**\n",
        "\n",
        "Black box distillation aims to learn from another model only from its output probability. We teach the student to reproduce these probabilities from the inputs.\n",
        "\n",
        "**Adavantages** :\n",
        "- **Simple to implement**: Query the teacher and train on its outputs. We don't need label at all as it is generated by the trained model itself.\n",
        "- **No knowledge needed on the model**: Doesn't require access to teacher's architecture or weights.\n",
        "- **Cross-architecture**: Can distill from any teacher model type to any student mdoel (Transformer to CNN for example).\n",
        "\n",
        "**Inconvenients** :\n",
        "- **Few information transfer**: Even if we take advantage of another model to train our (no need to have big dataset), we only learns from final outputs, missing rich intermediate representations.\n",
        "- **Limited guidance**: Can't leverage layer-wise or attention-based knowledge.\n",
        "\n",
        "\n",
        "### **White-box distillation**\n",
        "\n",
        "White box distillation allows a student model to learn from a teacher model that we fully knows (weigths, architecture, etc). Here, we want to teach a small teacher model, some characteristics of the bigger teacher model. For example, we create a student model with 3 layers (teacher with 5). Then, try to make the first student layer to learn the first teacher layer, the second student layer to learn the third teacher layer, then 3->5.\n",
        "\n",
        "**Adavantages** :\n",
        "- **Rich knowledge transfer**: Can match intermediate representations, not just final outputs.\n",
        "- **Efficient learning**: Student learns from multiple levels of abstraction simultaneously.\n",
        "- **Targeted distillation**: Can focus on specific components (attention, embeddings, specific layers).\n",
        "\n",
        "\n",
        "**Inconvenients** :\n",
        "- **Requires access**: Needs the teacher's weights and architecture.\n",
        "- **Architecture constraints**: Student and teacher often need compatible structures.\n",
        "- **High computational cost during distillation**: Must compute and match multiple representations."
      ],
      "metadata": {
        "id": "1EbhsdCNR3ME"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='red'>\n",
        "\n",
        "<hr style=\"border:10px solid red\"> </hr>  \n",
        "Question 2:</br>\n",
        "What is the main requirement for a teacher/student pair of models to perform white-box distillation?\n",
        "<hr style=\"border:10px solid red\"> </hr>  \n",
        "</font></h4>\n",
        "\n"
      ],
      "metadata": {
        "id": "2n3yni_LSvp6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 2: </b><br>\n",
        "Your answer here.\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>\n",
        "\n",
        "With white box distillation, we aim to reproduce the behavior of several component of the teacher model. In order to do it, we must have **architecture compatibility** in order to have some logic on the way the pattern are learned in the student model. We must be a way to map corresponding components between teacher and student.\n"
      ],
      "metadata": {
        "id": "exgIPD7kS6nd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 - Synthetic Data Generation\n",
        "\n",
        "We're going to specialize a small 0.5B parameter model to perform RAG by distilling the abilities of a 7B parameter one.\n",
        "\n",
        "For that we'll be using `Qwen/Qwen2.5-0.5B-Instruct` as student and `Qwen/Qwen2.5-7B-Instruct-AWQ` (quantized version of `Qwen2.5-7B-Instruct`) as teacher.  \n",
        "\n",
        "In order to perform white-box distillation on generated answers we have two choices.\n",
        "\n",
        "1. We can perform a forward pass with the teacher, a forward pass with student on the complete sequence, and backprop difference of logprobs using KL Loss.\n",
        "2. Generation of samples with the teacher, save the logprobs and perform finetuning in a second step."
      ],
      "metadata": {
        "id": "gqGfvYFWeOnv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='red'>\n",
        "\n",
        "<hr style=\"border:10px solid red\"> </hr>  \n",
        "Question 3:</br>\n",
        "What are the computational advantages of 1. vs 2.?\n",
        "<hr style=\"border:10px solid red\"> </hr>  \n",
        "</font></h4>"
      ],
      "metadata": {
        "id": "rBMX51LVXDpA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 3: </b><br>\n",
        "Your answer here.\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>\n",
        "\n",
        "The computational advantages of 1. is that we do not need to store a lot of data as we will only store the teacher and student response to a unique forward pass. The downside of it is that we have to wait for the inference of the teacher for each training step of the student, that can add a **huge amount of time especially** as the teacher is a bigger model than the student.\n",
        "\n",
        "In the same time, 2. has a clear downside. We need to **store a lot of samples from the teacher in memory**, that we will then use to fine the student. This approach is **way more computationally effective** as, once we have infer a lot of samples with the teacher, we only have to train the student by one student forward pass per step. This is a lot quicker than the 1. Moreover, the s**amples we have from the teacher can be reused** whenever we want.\n",
        "\n",
        "To conclude, we will prefer 1. if we have limited memory and 2. if we have a lot of it.\n"
      ],
      "metadata": {
        "id": "jL9i517XXIjx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're going to generate a bunch of questions related to wikipedia paragraphs.\n",
        "For that we need to establish a system prompt that will allow for easy extraction."
      ],
      "metadata": {
        "id": "mtUANmlPZ92m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "You are a question generator.\n",
        "The user will provide:\n",
        "\n",
        "```json\n",
        "{\"title\": \"the title of an article\", \"paragraph\": \"a paragraph from that article\"}\n",
        "```\n",
        "\n",
        "Your task:\n",
        "\n",
        "* Generate one clear, self-contained question that can be answered using only the provided paragraph.\n",
        "* The question must be **specific**, **unambiguous**, and directly tied to the paragraph’s content.\n",
        "* Return the result with the question as a valid JSON** in the form:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"question\": \"your question here\"\n",
        "}\n",
        "```\n",
        "\n",
        "Example:\n",
        "User input:\n",
        "\n",
        "```json\n",
        "{\n",
        "\"title\": \"The Moon Landing\",\n",
        "\"paragraph\": \"On July 20, 1969, Neil Armstrong became the first human to set foot on the Moon, followed by Buzz Aldrin.\"\n",
        "}\n",
        "```\n",
        "Assistant output:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"question\": \"Who was the first human to set foot on the Moon during the Apollo 11 mission?\"\n",
        "}\n",
        "```\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "d5X6h9urZ8yQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='red'>\n",
        "\n",
        "<hr style=\"border:10px solid red\"> </hr>  \n",
        "Question 4:</br>\n",
        "In this system prompt, we don't generate answers, only questions. Explain why it's necessary in the context of white-box distillation.\n",
        "<hr style=\"border:10px solid red\"> </hr>  \n",
        "</font></h4>"
      ],
      "metadata": {
        "id": "XJnC4cjJaRpI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 4: </b><br>\n",
        "Your answer here.\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>\n",
        "\n",
        "Here, we created a question generator. This, because we want to have a data/question dataset that are the same for the teacher and student. The teacher will then take this couple as an input and we will store the teacher data across the layers generated during the forward pass. Finally, giving the same data/question to the student, we will make it learn the teacher parameters from it."
      ],
      "metadata": {
        "id": "uo4p-YsrauUR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='red'>\n",
        "\n",
        "<hr style=\"border:10px solid red\"> </hr>  \n",
        "Question 5:</br>\n",
        "For synthetic data generation we'll be using vLLM.\n",
        "vLLM is an optimized llm inference engine that can improve generation speed thanks to hardware specific optimization and computational tricks such as Prefix KV Caching.\n",
        "(https://docs.vllm.ai/en/latest/features/automatic_prefix_caching.html)</br></br> 1. Explain why prefix caching will be very efficient in our case? </br></br>\n",
        "2. What sampling `temperature` should we use? Justify.\n",
        "\n",
        "\n",
        "\n",
        "<hr style=\"border:10px solid red\"> </hr>  \n",
        "</font></h4>\n"
      ],
      "metadata": {
        "id": "m1WuONVpdVvv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 5: </b><br>\n",
        "Your answer here.\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>\n",
        "\n",
        "### **1. Explain why prefix caching will be very efficient in our case?**\n",
        "\n",
        "Here, the question generator will generate questions from wikipedia articles from the same system prompt (its instructions). As this system prompt is always the same, and quite long, we do not want to compute attention from it every time as we will always have the sams values. Prefix caching allows us to cache the Key-Value matrices of this prompt. Then, we don't have to compute it for every steps, allowing us some real save in term of computational speed.\n",
        "\n",
        "\n",
        "### **2. What sampling `temperature` should we use? Justify.**\n",
        "We may use a temperature close to 0 in order to have what the teacher model will reponds in general. The higher is the temperature, the more we have the chance to have an atypical answer from the teacher model. Nevertheless, we don't necessarily want original answers here. We are more focus about transfering the general knowledge of the teacher to the student. A high temperature would create noise in the training and we don't want that.  \n",
        "\n",
        "For distillation, we want:\n",
        "\n",
        "- **Deterministic**, high-quality answers from the teacher that represent its \"best\" knowledge.\n",
        "- **Consistent behavior**. The teacher should give its most confident, accurate responses.\n",
        "- **Clear signal** for the student.\n",
        "\n",
        "In order to have all of this, **I will suggest a temperature around 0.2**. Not 0 because in that case, the teacher answers might be too specific and we have a risk of overfitting."
      ],
      "metadata": {
        "id": "XIazdZ4xd67Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 1: Optimized Generation of Synthetic Questions\n",
        "\n",
        "Complete the below code with the adequate options (prefix caching and `temperature`)\n",
        "\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>\n"
      ],
      "metadata": {
        "id": "LxRUkWoOcAcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from vllm import LLM\n",
        "from vllm import SamplingParams\n",
        "import os\n",
        "path_teacher = \"Qwen/Qwen2.5-7B-Instruct-AWQ\"\n",
        "llm = LLM(model=path_teacher, gpu_memory_utilization=0.7, max_model_len=5000, enable_prefix_caching=True)\n",
        "sampling_params = SamplingParams(temperature = 0.2, max_tokens=400)\n"
      ],
      "metadata": {
        "id": "IAfCQdVLayvA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305,
          "referenced_widgets": [
            "fe84f433a9714129a15ffb6d2166396a",
            "89a5e6e8674b43e188156e6f0f4d3466",
            "9eeed2ed02e2495fac33b80a02fd40e0",
            "785ff94615f9476794ba0d192cb0d6b2",
            "0523c003089b4757a5a5ec983b788c4a",
            "a6a81a1fcd964f0d82f89bc145e40ddf",
            "6fdb5dd4ea194ea9959fa73ccbeb0fe4",
            "5f7710e1e5ca4832bcb590bd4079703d",
            "df81d73394784b3c8e5f56d3657e9512",
            "c8a386585d764e2dbe536a4d2808616c",
            "fc85c0288e0b4e15bccd192772eb5870"
          ]
        },
        "outputId": "6a47ddcd-12a1-4047-f380-bd7fcfb9f31b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 11-23 22:37:49 [utils.py:253] non-default args: {'max_model_len': 5000, 'enable_prefix_caching': True, 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'model': 'Qwen/Qwen2.5-7B-Instruct-AWQ'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 11-23 22:37:51 [model.py:631] Resolved architecture: Qwen2ForCausalLM\n",
            "INFO 11-23 22:37:51 [model.py:1745] Using max model len 5000\n",
            "INFO 11-23 22:37:55 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Parse safetensors files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fe84f433a9714129a15ffb6d2166396a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 11-23 22:37:57 [system_utils.py:103] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
            "INFO 11-23 22:39:07 [llm.py:352] Supported tasks: ['generate']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 2: Question Generation</br>\n",
        "\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>\n",
        "\n",
        "We're going to use the llm.chat vLLM api to generate our samples.\n",
        "Make the adequate code to generate the data and save it in a questions.jsonl file.\n",
        "\n",
        "Entries of jsonl file should look like:\n",
        "```json\n",
        "{\n",
        "  \"id_doc\": <wikipedia_article_id>,\n",
        "  \"id_paragraph\": <paragraph_dataset_id>,\n",
        "  \"question\": <generated_question>,\n",
        "  \"title\": <title_wikipedia_article>,\n",
        "  \"paragraph\": <text_of_paragraph>\n",
        "}\n",
        "```\n",
        "\n",
        "You should:\n",
        "\n",
        "1. Complete `def extract_question(generated_text: str) -> str:` to extract the generated question.\n",
        "2. Complete `def conversation_generator` to output a generator directly ingestible by `llm.chat` api\n",
        "3. Complete the dataloader and for loop to generate samples by batches of 4\n",
        "4. Save the generated questions in a `questions.jsonl` file"
      ],
      "metadata": {
        "id": "AiYVPVzEegzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Iterable, Iterator, List\n",
        "import json\n",
        "def extract_question(generated_text: str) -> str:\n",
        "\n",
        "    \"\"\"Extract the question from the generated text. If the question is not\\\n",
        "    following the right format return None.\"\"\"\n",
        "\n",
        "    try:\n",
        "      j: dict\n",
        "      j = json.loads(generated_text)\n",
        "      assert \"question\" in j\n",
        "      return j[\"question\"]\n",
        "    except (AssertionError, json.JSONDecodeError):\n",
        "      return None\n",
        "\n",
        "\n",
        "def conversation_generator(\n",
        "    entries: Iterable[dict],\n",
        "    system_prompt: str\n",
        "    ) -> Iterator[dict]:\n",
        "\n",
        "    \"\"\"Generate the conversation with the model.\"\"\"\n",
        "\n",
        "    for entry in entries:\n",
        "      conversation: List[dict]\n",
        "      conversation = [\n",
        "          {\"role\": \"system\", \"content\": system_prompt},\n",
        "          {\"role\": \"user\", \"content\": json.dumps({\"title\": entry[\"title\"], \"id_doc\": entry[\"id\"], \"paragraph\": entry[\"paragraph\"]})}\n",
        "      ]\n",
        "      yield conversation"
      ],
      "metadata": {
        "id": "5-GzUDo9gxGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"EvanD/Lab4_wikiparagraphs\")\n",
        "\n",
        "batch_size = 4\n",
        "conversations = list(conversation_generator(ds[\"train\"], system_prompt))\n",
        "\n",
        "dataloader = DataLoader(conversations, batch_size=batch_size, shuffle=False, num_workers=2, prefetch_factor=2, collate_fn=lambda x: x)"
      ],
      "metadata": {
        "id": "ILcseIFmegZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds['train']['id']"
      ],
      "metadata": {
        "id": "uCx0RrMWivfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversations[0]"
      ],
      "metadata": {
        "id": "BGd9l83ohJV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "import jsonlines\n",
        "\n",
        "\n",
        "with jsonlines.open(\"questions.jsonl\", \"w\") as writer:\n",
        "    for i, batch in tqdm(enumerate(dataloader)):\n",
        "        id_paragraph = i * batch_size\n",
        "        outputs = llm.chat(batch,\n",
        "                        sampling_params=sampling_params,\n",
        "                        use_tqdm=False)\n",
        "        for j, output in enumerate(outputs):\n",
        "          entry = ds['train'][id_paragraph + j]\n",
        "          q = extract_question(output.outputs[0].text)\n",
        "\n",
        "          if q:\n",
        "            writer.write({\n",
        "                \"id_doc\": entry[\"id\"],\n",
        "                \"id_paragraph\": id_paragraph,\n",
        "                \"question\": q,\n",
        "                \"title\": entry[\"title\"],\n",
        "                \"paragraph\": entry[\"paragraph\"]\n",
        "            })"
      ],
      "metadata": {
        "id": "0ype0N4eedtJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "3a37b1ae794945b488f7aba65ec65b55",
            "e0859a00ac5946bfbf30227aaf22c66a",
            "7a70128d94554bb9a98da308dcdcc708",
            "548b0822e46b4c79b82bdd509ece656e",
            "370fb5ce556a48a29dab80411f1a2a4e",
            "96d2ebfe8d4c4e80968740080d025c29",
            "c1325fb7172241e0bd3984368a9137cc",
            "5730559795f74ec893c2ae72fb945e2d",
            "09bacea4ecb244a7b15928e6efd32b85",
            "22a8210d44054130bd040481a0623b3e",
            "009d99e75dde4466889dd5f33cf4f704"
          ]
        },
        "outputId": "54e7fdb3-21dd-4534-a523-82ebfbb01706"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a37b1ae794945b488f7aba65ec65b55"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 11-23 22:39:13 [chat_utils.py:557] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds['train'][0][\"paragraph\"]"
      ],
      "metadata": {
        "id": "Ndqiln0a2MKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1J2KNnI52G4G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 - Logprobs Generation\n",
        "\n",
        "We get to the second part of this distillation where we are interested in distilling the answers logprobs of our teacher model to specialize our 0.5B model to perform Retrieval Augmented Generation (RAG).\n",
        "\n",
        "First we're going to generate the logprobs with our 7B parameter model.\n",
        "\n",
        "We'll use the following system prompt:"
      ],
      "metadata": {
        "id": "kxDRanUZespX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = (\n",
        "    \"You are an assistant for a Retrieval-Augmented Generation (RAG) system.\\n\"\n",
        "    \"Answer the question using only the provided documents. \"\n",
        "    \"If the answer cannot be found in the provided documents, respond that the answer is not available in the provided document database. \"\n",
        "    \"Documents:\\n{context_block}\\n\\n\"\n",
        ")"
      ],
      "metadata": {
        "id": "XrSA_S5le2a5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 3: Complete Conversation Generator</br>\n",
        "\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>\n",
        "\n",
        "Based on the previous code you made to generate questions, make a second one to generate answers and saving logprobs. We updated the sampling parameters of vLLM to return the logprobs and the token ids of the 20 most probable tokens.\n",
        "\n",
        "As this can result in high quantity of data in practice we're going to store generated conversations in a .jsonl file and generated logprobs and token ids to a hdf5 file (see https://docs.h5py.org/en/stable/ for more information on hdf5).\n",
        "\n",
        "`save_logprobs_hdf5()` is already implemented for you, and allows to save the logprobs to a .h5 hdf5 file, and increments sequences automatically\n",
        "\n",
        "The structure of the conversation generator `conversation_generator()` function is implemented, you need to complete it.\n",
        "\n"
      ],
      "metadata": {
        "id": "QnYyOhFJv1qK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py, os\n",
        "import numpy as np\n",
        "\n",
        "def save_logprobs_hdf5(path, sequences, start_idx=None):\n",
        "    \"\"\"\n",
        "    sequences: list of sequences\n",
        "       each sequence = list of steps\n",
        "          each step = {token_id: Logprob(logprob=..., ...), ...}\n",
        "    \"\"\"\n",
        "    mode = \"a\" if os.path.exists(path) else \"w\"\n",
        "    with h5py.File(path, mode) as f:\n",
        "        # choose index to start writing\n",
        "        if start_idx is None:\n",
        "            # auto-continue numbering if file already has data\n",
        "            existing = [int(k.split(\"_\")[1]) for k in f.keys() if k.startswith(\"seq_\")]\n",
        "            start_idx = max(existing)+1 if existing else 0\n",
        "\n",
        "        for s_i, seq in enumerate(sequences, start=start_idx):\n",
        "            g = f.create_group(f\"seq_{s_i}\")\n",
        "            for t_i, step in enumerate(seq):\n",
        "                token_ids = np.fromiter(step.keys(), dtype=np.int32)\n",
        "                logprobs  = np.array([lp.logprob for lp in step.values()], dtype=np.float32)\n",
        "                g.create_dataset(f\"step_{t_i}/token_ids\", data=token_ids, compression=\"gzip\")\n",
        "                g.create_dataset(f\"step_{t_i}/logprobs\",  data=logprobs,  compression=\"gzip\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kFquvqqivjs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='red'>\n",
        "\n",
        "<hr style=\"border:10px solid red\"> </hr>  \n",
        "Question 6:</br>\n",
        "When completing conversation_generator we have several ways of sampling.\n",
        "What are good paragraph sampling strategies we could use to ensure good performance of downstream model?\n",
        "\n",
        "<hr style=\"border:10px solid red\"> </hr>  \n",
        "</font></h4>\n"
      ],
      "metadata": {
        "id": "HxjgwhDhzzM-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 6: </b><br>\n",
        "Your answer here.\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>"
      ],
      "metadata": {
        "id": "l7JIzEqU0Q91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "doc_id_to_paragraphs = {}\n",
        "\n",
        "for line in ds[\"train\"]:\n",
        "    doc_id = line[\"id\"]\n",
        "    paragraph = line[\"paragraph\"]\n",
        "    title = line[\"title\"]\n",
        "    if doc_id not in doc_id_to_paragraphs:\n",
        "        doc_id_to_paragraphs[doc_id] = []\n",
        "    doc_id_to_paragraphs[doc_id].append(title + \" -- \" + paragraph)  # We add the title to contextualize the paragraph\n",
        "\n",
        "def conversation_generator(\n",
        "    path_jsonl: str,\n",
        "    system_prompt: str,\n",
        "    top_k: int = 3\n",
        "    ) -> Iterator[dict]:\n",
        "\n",
        "    \"\"\"Generate the conversation with the model.\"\"\"\n",
        "\n",
        "    with jsonlines.open(path_jsonl, \"r\") as f:\n",
        "      for q_p in f:\n",
        "        number_of_paragraphs_in_context = random.sample(range(1, top_k + 1), 1)[0]\n",
        "        paragraphs = [q_p[\"title\"] + \" -- \" + q_p[\"paragraph\"]] # We add the title to contextualize the paragraph\n",
        "        while len(paragraphs) < number_of_paragraphs_in_context:\n",
        "          # With 50% probability, add a paragraph from the same article\n",
        "          if random.random() < 0.5 and q_p[\"id\"] in doc_id_to_paragraphs:\n",
        "            same_doc_paragraphs = doc_id_to_paragraphs[q_p[\"id\"]]\n",
        "            candidates = [p for p in same_doc_paragraphs if p != q_p[\"title\"] + \" -- \" + q_p[\"paragraph\"]]\n",
        "            if candidates:\n",
        "                paragraphs.append(random.choice(candidates))\n",
        "                continue\n",
        "\n",
        "        # Add a paragraph from a different article\n",
        "        different_doc_id = random.choice([doc_id for doc_id in doc_id_to_paragraphs.keys() if doc_id != q_p[\"id\"]])\n",
        "        paragraphs.append(random.choice(doc_id_to_paragraphs[different_doc_id]))\n",
        "\n",
        "\n",
        "\n",
        "        random.shuffle(paragraphs)\n",
        "        system_prompt_formatted = system_prompt.format(\n",
        "            context_block=\"\\n\\n\".join(f\"[Document {i+1}]: {doc}\" for i, doc in enumerate(paragraphs))\n",
        "        )\n",
        "        conversation = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt_formatted},\n",
        "            {\"role\": \"user\", \"content\": q[\"question\"]}\n",
        "        ]\n",
        "        yield conversation\n",
        "\n"
      ],
      "metadata": {
        "id": "AX-wTajUzH-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "from typing import Iterable\n",
        "\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "conversations = list(conversation_generator(path_jsonl=\"questions.jsonl\", system_prompt=system_prompt)) # To Complete\n",
        "\n",
        "dataloader = DataLoader(conversations, batch_size=batch_size, shuffle=False, num_workers=2, prefetch_factor=2, collate_fn=lambda x: x)\n",
        "\n",
        "sampling_params = SamplingParams(temperature=0.2, max_tokens=400, logprobs=20)\n",
        "\n",
        "with jsonlines.open(\"conversations_rag.jsonl\", \"w\") as writer:\n",
        "    for batch in tqdm(dataloader):\n",
        "        out = llm.chat(batch,\n",
        "                        sampling_params=sampling_params,\n",
        "                        use_tqdm=False)\n",
        "        for i, b in enumerate(batch):\n",
        "            text = out[i].outputs[0].text\n",
        "            b.append({\"role\": \"assistant\", \"content\": text})\n",
        "            writer.write(b)\n",
        "        save_logprobs_hdf5(\"logprobs.h5\", [out[i].outputs[0].logprobs for i in range(len(out))])\n",
        "\n",
        "'''\n",
        "conversations = list(conversation_generator(path_jsonl=\"questions.jsonl\", system_prompt=system_prompt))\n",
        "'''"
      ],
      "metadata": {
        "id": "tUdhF7ZTz1ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 - KL-Divergence and Distillation\n",
        "\n",
        "**You should restart the notebook kernel to free the gpu memory from the 7B model that is no longer needed**\n",
        "\n",
        "Now that we have the generated conversations and their logprobs we can train our 0.5B model to output the same distribution.\n",
        "\n",
        "The attentive student would have noticed that we have an incomplete representation of the probability distribution over tokens due to only keeping the top-20 logprobs.\n"
      ],
      "metadata": {
        "id": "81G0b7cS01nX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='red'>\n",
        "\n",
        "<hr style=\"border:10px solid red\"> </hr>  \n",
        "Question 7:</br>\n",
        "What are the two solutions you can see to approximate full distillation despite only having the top-20 logprobs?\n",
        "\n",
        "<hr style=\"border:10px solid red\"> </hr>  \n",
        "</font></h4>"
      ],
      "metadata": {
        "id": "TZQCifoJ4ml-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 7: </b><br>\n",
        "Your answer here.\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>\n",
        "\n",
        "The issue is that limiting the teacher's output distribution to only the top-20 most probable tokens means we are ignoring the small, non-zero probabilities for the rest of the vocabulary.\n",
        "\n",
        "\n",
        "Here are two solutions to approximate full distillation despite this limitation:\n",
        "\n",
        "\n",
        "### **1. Zero-Out Missing Probabilities**:\n",
        "\n",
        "\n",
        "Treat all tokens outside the teacher's top-20 list for a given time step as having a probability of 0 and log-probability of log(0)=−∞ (or the minimum finite value). We then have a simplified KL divergence calculation by only summing over the tokens in the top-20 list.\n",
        "\n",
        "\n",
        "### **2. Adding a Uniform Probability Mass to the Tail**:\n",
        "\n",
        "\n",
        "Estimate the residual probability mass as a uniform law on the rest of the tokens. Distribute this residual mass uniformly among the remaining tokens in the vocabulary. This provides a non-zero, consistent soft target for the tail tokens, making the KL divergence calculation more accurate."
      ],
      "metadata": {
        "id": "9WrDEzqJ43cG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We supply two functions to help in this implementation:\n",
        "\n",
        "- `find_subsequence()`that allows to find all the occurences of a token subsequence in a 1-D tensor\n",
        "- `get_labels()` that allows to expand the top-20 logprobs to the whole vocabulary, implicitly setting prob to zero for other tokens\n",
        "- `QwenKLDataset`that loads samples from .h5 and .jsonl files, remove problematic inconsistent tokenized examples and outputs samples tokenized for training.\n",
        "\n",
        "To simplify we will train with a batch size of 1, you can implement gradient accumulation if you wish.\n",
        "\n",
        "The `PREFIX_ASSISTANT`variable contains the tokens that encode for"
      ],
      "metadata": {
        "id": "I4W9x0NZ5OoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")"
      ],
      "metadata": {
        "id": "PLCK3P-59acl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def find_subsequence(input_ids: torch.Tensor, subseq: torch.Tensor):\n",
        "    \"\"\"Find the first index of each occurence of a subsequence in the input_ids.\"\"\"\n",
        "\n",
        "    subseq_len = len(subseq)\n",
        "    matches = []\n",
        "    for idx in range(input_ids.size(0) - subseq_len + 1):\n",
        "        if torch.equal(input_ids[idx:idx + subseq_len], subseq):\n",
        "            return [idx]\n",
        "    return []\n",
        "\n",
        "def get_labels(logprobs: torch.Tensor, tokens, size_vocab: int = 151936, offset: int = 0):\n",
        "    \"\"\"Get the greedy max probability tokenized sequence from the logprobs.\"\"\"\n",
        "    labels = torch.full((len(logprobs), size_vocab), torch.finfo(torch.float16).min, dtype=torch.float16)\n",
        "    for idx, logprobs_token in enumerate(logprobs):\n",
        "        for token_id, logprob in zip(tokens[idx], logprobs_token):\n",
        "            labels[idx][token_id] = logprob\n",
        "    return labels"
      ],
      "metadata": {
        "id": "fhqXsxRQ6jgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import jsonlines\n",
        "import h5py\n",
        "\n",
        "\n",
        "PREFIX_ASSISTANT = [198, 151644, 77091, 198]\n",
        "\n",
        "class QwenKLDataset(Dataset):\n",
        "    \"\"\"Dataset for finetuning Qwen model with KL divergence loss.\"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            path_h5,\n",
        "            path_jsonl\n",
        "        ):\n",
        "        self.entries = []\n",
        "        with jsonlines.open(path_jsonl, \"r\") as reader:\n",
        "            with h5py.File(path_h5, \"r\") as f:\n",
        "                for j, line in tqdm(enumerate(reader)):\n",
        "                    inputs = tokenizer.apply_chat_template(line, add_generation_prompt=False, tokenize=True, return_dict=True, return_tensors=\"pt\")\n",
        "                    idx_subseq = find_subsequence(inputs[\"input_ids\"][0], subseq = torch.tensor(PREFIX_ASSISTANT))[0]\n",
        "                    seq_f = [f[f\"seq_{j}\"][f\"step_{i}\"][\"token_ids\"][0] for i in range(len(f[f\"seq_{j}\"]))]\n",
        "                    try:\n",
        "                        assert len(inputs[\"input_ids\"][0][idx_subseq+3:-2]) == len(seq_f)\n",
        "                    except AssertionError:\n",
        "                        print(f\"AssertionError {j}, skipping inconsistent detokenization/tokenization\")\n",
        "                    tokens = [f[f\"seq_{j}\"][f\"step_{i}\"][\"token_ids\"][:] for i in range(len(f[f\"seq_{j}\"]))]\n",
        "                    logprobs = [f[f\"seq_{j}\"][f\"step_{i}\"][\"logprobs\"][:] for i in range(len(f[f\"seq_{j}\"]))]\n",
        "                    inputs[\"input_ids\"] = inputs[\"input_ids\"].cuda()\n",
        "                    self.entries.append(\n",
        "                        {\n",
        "                            \"inputs\": inputs,\n",
        "                            \"idx_subseq\": idx_subseq + 3,\n",
        "                            \"seq_len\": len(seq_f),\n",
        "                            \"logprobs\": torch.tensor(np.array(logprobs)),\n",
        "                            \"tokens\": torch.tensor(np.array(tokens))\n",
        "                        }\n",
        "                    )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.entries)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Return for a given entry:\n",
        "\n",
        "        new_tokens: torch.Tensor, the tokenized conversation with the logprobs of the assistant answer inserted.\n",
        "        idx_match: int, the index at which the assistant answer starts in the new_tokens.\n",
        "        len_logprob_sequence: int, the tokenized length of the assistant answer.\n",
        "        labels: torch.Tensor, the logprobs of the assistant answer for each token.\n",
        "        \"\"\"\n",
        "        entry = self.entries[idx]\n",
        "        entry[\"labels\"] = get_labels(entry[\"logprobs\"], entry[\"tokens\"], offset=0, size_vocab=151936).cuda()\n",
        "\n",
        "        return entry"
      ],
      "metadata": {
        "id": "b2B1QbHo6kd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KL-Divergence is a metric often used to quantify the difference of probability mass between two distributions.\n",
        "It's not mathematically defined as a distance because of its asymetric nature:\n",
        "\n",
        "$$D_{KL}(P \\parallel Q) = \\sum_{x} P(x) \\log \\frac{P(x)}{Q(x)}$$\n",
        "\n",
        "In knowledge distillation, we often minimize the **Kullback–Leibler divergence** between the teacher’s output distribution $P_T$ and the student’s output distribution $P_S$.\n",
        "\n",
        "$$\n",
        "D_{KL}(P_T \\parallel P_S) = \\sum_x P_T(x) \\log \\frac{P_T(x)}{P_S(x)}\n",
        "$$\n",
        "$$\n",
        "D_{KL}(P_S \\parallel P_T) = \\sum_x P_S(x) \\log \\frac{P_S(x)}{P_T(x)}\n",
        "$$\n",
        "\n",
        "\n",
        "<b><h4><font color='red'>\n",
        "\n",
        "<hr style=\"border:10px solid red\"> </hr>  \n",
        "Question 8:</br>\n",
        "What qualitative difference would it make if we minimize $D_{KL}(P_T \\parallel P_S)$ instead of $D_{KL}(P_S \\parallel P_T)$? How would it affect the convergence of the student distribution?\n",
        "\n",
        "<hr style=\"border:10px solid red\"> </hr>  \n",
        "</font></h4>\n",
        "\n"
      ],
      "metadata": {
        "id": "PaREIiPaENzm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 8: </b><br>\n",
        "Your answer here.\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>\n",
        "\n",
        "\n",
        "**DKL(PS∥PT)** :\n",
        "\n",
        "It is also nammed \"Mode Covering\" in the litterature. When PS has probability but PT doesn't → high penalty. When PT has probability but PS doesn't → low penalty\n",
        "\n",
        "Behavior: Student tries to cover all modes where teacher has probability. The student spreads its probability mass to avoid missing anything the teacher considers likely. Student distribution is often more diffuse/broad than teacher, covering multiple possible outputs even if some are low probability.\n",
        "\n",
        "\n",
        "**DKL(PT∥PS)** :\n",
        "\n",
        "Also nammed \"Mode Seeking\" in the litterature. When PT has probability but PS doesn't → high penalty. When PS has probability but PT doesn't → low penalty\n",
        "\n",
        "Behavior: Student tries to focus on the main modes of the teacher. The student concentrates probability mass on the teacher's highest-probability regions.\n",
        "Result: Student distribution is often more peaked/sharp, focusing on the teacher's most confident predictions and ignoring low-probability alternatives.\n",
        "Effect on Convergence\n",
        "\n",
        "**For distillation**, we will prefer to use DKL(PS∥PT) as we want the student to capture the full richness of the teacher's knowledge, not just its most confident predictions."
      ],
      "metadata": {
        "id": "cV18ExQdISvW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**You need to ensure you restarted the kernel and have sufficiently free memory (no 7B model running)**\n",
        "\n",
        "check with `! nvidia-smi`"
      ],
      "metadata": {
        "id": "h0Sv8L5vLIYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! nvidia-smi"
      ],
      "metadata": {
        "id": "PtU5tNhOLMK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "This code is very basic, and allows you to test a training over the small number of samples you've generated. Due to limitations of Google Colab we cannot go much further, but this first part should have given you the basics on how to perform white-box distillation.\n",
        "\n",
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 4: Complete The Training Code</br>\n",
        "\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>\n",
        "\n",
        "\n",
        "- Loss function\n",
        "- gradient accumulation handling\n",
        "\n"
      ],
      "metadata": {
        "id": "cxzgt4p5IzYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "student_model = None # To Complete\n",
        "dataset = None # To Complete\n",
        "\n",
        "num_epochs = 2\n",
        "grad_accum_steps = 8\n",
        "optimizer = torch.optim.AdamW(student_model.parameters(), lr=5e-5)\n",
        "loader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=lambda x: x[0])\n",
        "num_training_steps = num_epochs * len(loader) // grad_accum_steps\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, 0, num_training_steps)\n",
        "loss_fn = None # To Complete\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, batch in enumerate(loader):\n",
        "        outputs = student_model(batch[\"inputs\"][\"input_ids\"])\n",
        "        # To Complete\n",
        "        if (i + 1) % grad_accum_steps == 0 or (i + 1) == len(loader):\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            print (f\"Epoch {epoch+1}, step {i+1}/{len(loader)}: loss = {batch_loss.item():.4f}\")\n",
        "    print(f\"Epoch {epoch+1}: loss = {outputs.loss.item():.4f}\")\n",
        "\n",
        "student_model.save_pretrained(\"finetuned_model\")\n",
        "tokenizer.save_pretrained(\"finetuned_model\")\n"
      ],
      "metadata": {
        "id": "N4c-zOROIZ8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2 - Retrieval Augmented Generation (RAG)\n",
        "\n",
        "In this section, we will discuss the concept of **Retrieval-Augmented Generation (RAG)** — a framework that combines **information retrieval** and **language generation**. RAG enables language models to access **external knowledge sources** at inference time, reducing hallucinations and improving factual accuracy.\n",
        "\n",
        "We will explore how to:\n",
        "- Build and index a **Vector database** from a corpus (here: Wikipedia sample).\n",
        "- Retrieve the most relevant documents given a query using **embedding-based similarity**.\n",
        "- Integrate retrieval results into the **generation pipeline** to produce context-aware answers.\n"
      ],
      "metadata": {
        "id": "uECS-Qh7dv28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# restart the session and run\n",
        "!pip -q install chromadb==0.4.22\n",
        "!pip -q install \"numpy<2.0\" --force-reinstall\n",
        "!pip -q install datasets==2.21.0 pandas==2.2.2"
      ],
      "metadata": {
        "id": "qwkhRwYVyYf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "from IPython.display import display, HTML\n",
        "from transformers import AutoTokenizer, AutoModel"
      ],
      "metadata": {
        "id": "lwhx5lJ2TQxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "SEED = 42\n",
        "NUM_ROWS = 1000\n",
        "DATA_PATH = \"wikipedia_20231101_en_1000.csv\"\n",
        "\n",
        "if os.path.exists(DATA_PATH):\n",
        "    print(f\"✅ Found existing dataset at {DATA_PATH}\")\n",
        "    df = pd.read_csv(DATA_PATH)\n",
        "else:\n",
        "    print(\"⏳ Generating new dataset from Wikimedia (English, 2023-11-01)...\")\n",
        "    random.seed(SEED)\n",
        "\n",
        "    # Load the Wikipedia dataset\n",
        "    stream_ds = load_dataset(\n",
        "        \"wikimedia/wikipedia\",\n",
        "        \"20231101.en\",\n",
        "        split=\"train\",\n",
        "        streaming=True\n",
        "    )\n",
        "\n",
        "    buffered_stream = stream_ds.shuffle(seed=SEED, buffer_size=200_000)\n",
        "\n",
        "    sampled = []\n",
        "    for ex in buffered_stream:\n",
        "        try:\n",
        "            if int(ex[\"id\"]) % 2 == 0:\n",
        "                sampled.append(ex)\n",
        "            if len(sampled) >= NUM_ROWS:\n",
        "                break\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(sampled)[[\"id\", \"url\", \"title\", \"text\"]]\n",
        "    df.to_csv(DATA_PATH, index=False)\n",
        "    print(f\"💾 Dataset saved to {DATA_PATH}\")\n"
      ],
      "metadata": {
        "id": "iRW9L9s4dbeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Display Basic Info ---\n",
        "print(\"Sampled shape:\", df.shape)\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "display(df.head())"
      ],
      "metadata": {
        "id": "kfqExXsQaxGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize Random Wikipedia Articles ---\n",
        "\n",
        "NUM_EXAMPLES = 3  # number of random samples to show\n",
        "samples = df.sample(NUM_EXAMPLES, random_state=random.randint(0, 10000))\n",
        "\n",
        "for _, row in samples.iterrows():\n",
        "    display(HTML(f\"\"\"\n",
        "    <hr style=\"border:2px solid #ccc\">\n",
        "    <h3><b>Title:</b> {row['title']}</h3>\n",
        "    <p><b>URL:</b> <a href=\"{row['url']}\" target=\"_blank\">{row['url']}</a></p>\n",
        "    <p style=\"text-align: justify;\"><b>Text:</b><br>{row['text']}</p>\n",
        "    \"\"\"))\n"
      ],
      "metadata": {
        "id": "foHQBZ7556Ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Document Chunking**\n",
        "\n",
        "The first step in building a RAG pipeline is **chunking**, where large documents are divided into smaller, semantically coherent pieces.  \n",
        "Chunking allows the retriever to work on manageable text segments instead of entire documents, improving retrieval precision and reducing computational load.  \n"
      ],
      "metadata": {
        "id": "H0rj2jrugW1H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 1: Naive Fixed-Length Chunking  \n",
        "Split each document into overlapping fixed-length chunks to prepare text for retrieval.\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>\n"
      ],
      "metadata": {
        "id": "Zxsosg3b99AH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_splitting(text, chunk_length=300, chunk_overlap=100):\n",
        "    \"\"\"\n",
        "    Splits text into fixed-length chunks with overlap.\n",
        "    \"\"\"\n",
        "    out = []\n",
        "    stride = chunk_length - chunk_overlap ## FILL THE GAP: define the stride as the effective step between chunks\n",
        "    for i in range(0, len(text), stride):\n",
        "        chunk = text[i:i + chunk_length] ## FILL THE GAP: extract a substring of size 'chunk_length' starting at 'i'\n",
        "        out.append(chunk)\n",
        "    return out\n",
        "\n",
        "# Apply to all documents\n",
        "df[\"naive_chunks\"] = df[\"text\"].apply(lambda t: text_splitting(t, chunk_length=300, chunk_overlap=100))\n",
        "\n",
        "num_chunks = df[\"naive_chunks\"].apply(len)\n",
        "print(f\"Average number of chunks per document: {num_chunks.mean():.2f}\")\n",
        "print(f\"Total number of chunks: {num_chunks.sum()}\")\n",
        "\n",
        "example_idx = 0\n",
        "print(\"\\n--- Example document ---\")\n",
        "print(\"Title:\", df.iloc[example_idx][\"title\"])\n",
        "print(\"Original length:\", len(df.iloc[example_idx][\"text\"]))\n",
        "print(\"Number of chunks:\", len(df.iloc[example_idx][\"naive_chunks\"]))\n",
        "print(\"\\nFirst 2 chunks:\\n\")\n",
        "for i, c in enumerate(df.iloc[example_idx][\"naive_chunks\"][:2]):\n",
        "    print(f\"Chunk {i+1}:\\n{c[:400]}\\n{'-'*80}\")"
      ],
      "metadata": {
        "id": "CK5jseiB1n6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 2: Paragraph-Aware Chunking  \n",
        "Implement a smarter chunking strategy by using the ('.') as a boundary to split text into sentences or short paragraphs, then regroup them until reaching the desired chunk length.\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>\n"
      ],
      "metadata": {
        "id": "ntVGPW2x-sBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_splitting_paragraph(text, chunk_length=300):\n",
        "    \"\"\"\n",
        "    Splits text by sentences/paragraphs (using '.' as boundary)\n",
        "    and groups them until reaching the desired chunk length.\n",
        "    \"\"\"\n",
        "    out = []\n",
        "    paragraph_list = text.split('. ')## FILL THE GAP: split the text into smaller parts using '.' as a separator\n",
        "    current_text = \"\"\n",
        "    length = 0\n",
        "    for par in paragraph_list:\n",
        "        if length != 0 and length + len(par) < chunk_length:\n",
        "            current_text += '. ' + par ## FILL THE GAP: extend the ongoing chunk with the next segment\n",
        "            length += len(par) ## FILL THE GAP: increment the total length accordingly\n",
        "        else:\n",
        "            if len(current_text) != 0:\n",
        "                out.append(current_text) ## FILL THE GAP: store the current completed chunk before starting a new one)\n",
        "            current_text = par ## FILL THE GAP: initialize a new chunk with the current paragraph\n",
        "            length = len(current_text) ## FILL THE GAP: reset the chunk length counter\n",
        "    if len(current_text) > 0:\n",
        "        out.append(current_text) ## FILL THE GAP: add the last remaining chunk to the list)\n",
        "    return out\n",
        "\n",
        "\n",
        "# Apply to all documents\n",
        "df[\"paragraph_chunks\"] = df[\"text\"].apply(lambda t: text_splitting_paragraph(t, chunk_length=300))\n",
        "\n",
        "# Compute stats\n",
        "num_chunks_par = df[\"paragraph_chunks\"].apply(len)\n",
        "print(f\"Average number of paragraph-based chunks per document: {num_chunks_par.mean():.2f}\")\n",
        "print(f\"Total number of paragraph-based chunks: {num_chunks_par.sum()}\")\n",
        "\n",
        "# Example comparison\n",
        "example_idx = 432 #Check out other examples\n",
        "print(\"\\n--- Example document ---\")\n",
        "print(\"Title:\", df.iloc[example_idx]['title'])\n",
        "print(\"Original length:\", len(df.iloc[example_idx]['text']))\n",
        "print(f\"Character based chunks: {len(df.iloc[example_idx]['naive_chunks'])}\")\n",
        "print(f\"Paragraph based chunks: {len(df.iloc[example_idx]['paragraph_chunks'])}\")\n",
        "\n",
        "print(\"\\nParagraph chunk preview:\\n\")\n",
        "for i, c in enumerate(df.iloc[example_idx]['paragraph_chunks'][:6]):\n",
        "    print(f\"Chunk {i+1}:\\n{c[:400]}\\n{'-'*80}\")\n"
      ],
      "metadata": {
        "id": "EXKREfWx1n8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='red'>\n",
        "\n",
        "<hr style=\"border:10px solid red\"> </hr>  \n",
        "Question 1:  \n",
        "In the paragraph-aware chunking method above, we simply split Wikipedia text using the '.' delimiter to approximate sentence boundaries.  \n",
        "Discuss whether this is an effective strategy for creating meaningful chunks in a RAG system.\n",
        "Propose one or more improved chunking strategies that could better capture document structure — and you may include code snippets to justify or demonstrate your approach.  \n",
        "<hr style=\"border:10px solid red\"> </hr>  \n",
        "</font></h4>\n"
      ],
      "metadata": {
        "id": "XMtzpHHADf9x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 1: </b><br>\n",
        "Your answer here.\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>\n",
        "\n",
        " **Limitations:**\n",
        "Splitting text using the \".\" delimiter to approximate sentence boundaries is a naive approach and has several limitations in the context of a RAG system. Indeed, not all periods \".\" indicate the end of a sentence. Abbreviations (e.g., \"Dr.\", \"etc.\") and decimal numbers can lead to incorrect splits, resulting in fragmented chunks. We also loose context when using the \".\" delimiter. It can break logical flows, especially in complex or technical texts. Sentences are often contextually linked.\n",
        "\n",
        " **Alternative:**\n",
        "Instead of splitting only at ., split according to a fix chunk size or at paragraph boundaries (using \\n\\n tags for example) and ensure chunks have a small overlap to maintain context and this small overlap is a hyperparameter that we can tune."
      ],
      "metadata": {
        "id": "gL03t4mRDktx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='red'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Question 2: Consider a scenario where you want to perform RAG on source code (e.g., Python files, Java classes) instead of natural language text. Would the chunking methods demonstrated above (character-based and sentence/paragraph based with boundaries) work effectively for code? Explain why or why not, and describe how you would approach chunking source code to maintain semantic coherence. What specific characteristics of code structure would you need to consider?\n",
        "\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "</font></h4>"
      ],
      "metadata": {
        "id": "HHpQ-hmuDslc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 2: </b><br>\n",
        "Your answer here.\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>\n",
        "\n",
        "The chunking method demonstrated above is efficient only for character-based and sentence/paragraph based with boundaries, not effective for chunking source code. Chunking source code requires language aware parsing and an understanding of code structure. Unlike natural language, code cannot be split arbitrarily chunks must be syntactically valid and semantically coherent. We can identify logical blocks (functions, classes, loops), then chunk at block boundaries to ensure syntactic completeness, also add overlaps for shared variables or logic and finally include imports in the first chunk or as needed."
      ],
      "metadata": {
        "id": "su69vspNDtfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving chunks ---\n",
        "\n",
        "# Flatten chunks into a new DataFrame\n",
        "records = []\n",
        "for _, row in df.iterrows():\n",
        "    doc_id = row[\"id\"]\n",
        "    title = row[\"title\"]\n",
        "    url = row[\"url\"]\n",
        "    for i, chunk in enumerate(row[\"paragraph_chunks\"]):\n",
        "        records.append({\n",
        "            \"doc_id\": doc_id,\n",
        "            \"title\": title,\n",
        "            \"url\": url,\n",
        "            \"chunk_id\": f\"{doc_id}_chunk_{i}\",\n",
        "            \"chunk_text\": chunk.strip()\n",
        "        })\n",
        "\n",
        "# Create the flattened chunks DataFrame\n",
        "chunks_df = pd.DataFrame(records)\n",
        "print(f\"Total chunks: {len(chunks_df)}\")\n",
        "print(f\"Average chunk length: {chunks_df['chunk_text'].apply(len).mean():.2f} characters\\n\")\n",
        "\n",
        "# Show example\n",
        "print(\"Example rows:\")\n",
        "display(chunks_df.head())\n",
        "\n",
        "chunks_df.to_csv(\"wikipedia_chunks.csv\", index=False)\n",
        "print(\"Chunks saved to 'wikipedia_chunks.csv'\")\n",
        "print(f\"Total chunks: {len(chunks_df)}\")\n"
      ],
      "metadata": {
        "id": "G2kXurtV_Z45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Part II: Embedding</b>\n",
        "\n",
        "After chunking our documents, the next step is to convert text chunks into vector representations (embeddings). These embeddings capture the semantic meaning of the text in a high-dimensional space, allowing us to measure similarity between chunks and queries mathematically.\n",
        "\n",
        "We will use **sentence-transformers/all-MiniLM-L6-v2**, a compact and efficient embedding model that produces 384-dimensional embeddings for English text. This model offers a strong balance between performance and computational efficiency, making it well-suited for our RAG pipeline.\n"
      ],
      "metadata": {
        "id": "1u01gpv68UrU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 3: </b><br>\n",
        "Fill in the <code>embed()</code> function to encode text chunks and generate normalized embeddings using <code>sentence-transformers/all-MiniLM-L6-v2</code>.  \n",
        "Then, apply it to all documents in <code>chunks_df</code> and store the results.\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>\n"
      ],
      "metadata": {
        "id": "MZYveYQWfK-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2.3 Embedding Generation ---\n",
        "# Load model\n",
        "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(f\"Loaded embedding model: {model_name}\")\n",
        "\n",
        "\n",
        "# --- Define embedding function ---\n",
        "def embed(text_list, doc_type=\"document\"):\n",
        "    \"\"\"\n",
        "    Encodes a list of texts and returns normalized embeddings.\n",
        "    \"\"\"\n",
        "    encoded = tokenizer(\n",
        "        [f\"search_{doc_type}: {t}\" for t in text_list],\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(**encoded) ## FILL THE GAP: forward pass through the model to obtain hidden states\n",
        "        token_embeddings = output.last_hidden_state ## FILL THE GAP: extract the last hidden state from the output\n",
        "        pooled = torch.mean(token_embeddings, dim=1) ## FILL THE GAP: aggregate token embeddings (e.g., by summing along sequence dimension)\n",
        "        pooled = torch.nn.functional.normalize(pooled, p=2, dim=1) ## FILL THE GAP: apply L2 normalization along the embedding dimension\n",
        "    return pooled.cpu()\n"
      ],
      "metadata": {
        "id": "7S2wJvk9bW6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Test with one example ---\n",
        "sample_text = [\"Artificial intelligence is transforming the world.\"]\n",
        "sample_emb = embed(sample_text)\n",
        "print(f\"Sample embedding shape: {sample_emb.shape}\")\n",
        "\n",
        "# --- Apply to all chunks ---\n",
        "print(f\"\\nGenerating embeddings for {len(chunks_df)} chunks...\")\n",
        "\n",
        "emb_list = []\n",
        "for i in tqdm(range(0, len(chunks_df), 32)):\n",
        "    batch = chunks_df[\"chunk_text\"].iloc[i:i+32].tolist()\n",
        "    emb = embed(batch, doc_type=\"document\")\n",
        "    emb_list.append(emb)\n",
        "\n",
        "chunk_embeddings = torch.cat(emb_list, dim=0).numpy()\n",
        "chunks_df[\"embedding\"] = list(chunk_embeddings)\n",
        "\n",
        "print(\"\\nEmbeddings generated and added to DataFrame.\")\n",
        "print(chunks_df[[\"chunk_id\", \"title\", \"embedding\"]].head())"
      ],
      "metadata": {
        "id": "7xi2k9PQsbXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='red'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Question 3: In most retrieval systems, embeddings are represented as fixed-size vectors. Can you think of a way to design embeddings that can flexibly adjust their size or level of detail while still preserving meaningful similarity between representations? How could such an approach benefit Retrieval-Augmented Generation (RAG) systems in practice, particularly for improving efficiency or adapting to different computational budgets?\n",
        "\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "<i>Hint:</i> You can find the answer in the paper <a href=\"https://arxiv.org/pdf/2205.13147\" target=\"_blank\">Matryoshka Representation Learning</a>.\n",
        "</font></h4>\n"
      ],
      "metadata": {
        "id": "AX7tYUGYGBTm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In most retrieval systems, embeddings are represented as fixed-size vectors.\n",
        "The Matryoshka Representation Learning (MRL) offers a solution for flexible embeddings by training models with a multi-scale loss function. It ensures truncated prefixes of the full embedding vector maintain meaningful semantic information at multiple granularities simultaneously.\n",
        "\n",
        "The model is optimized such that the first m dimensions of a d-dimensional embedding each preserve useful similarity relationships. The most critical information is encoded in early dimensions and progressively finer details in later ones. For RAG systems, this enables cascaded retrieval where low-dimensional embeddings rapidly screen millions of candidates. It is then followed by higher-dimensional refinement on reduced sets, achieving 10-50× speedup while maintaining retrieval quality.\n",
        "\n",
        "This approach provides dynamic resource allocation based on computational resources, and it also reduces storage costs in vector databases. It enables efficient deployment across devices with varying capabilities. This make RAG systems more scalable, efficient, and practical for real-world applications without sacrificing semantic fidelity."
      ],
      "metadata": {
        "id": "imq9sOGM4hyS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Building a Simple Vector Database**\n",
        "\n",
        "After generating embeddings for all our document chunks, the next step is to **store** them in a structure that allows fast similarity search.  \n",
        "In this section, we will build a **simple in-memory vector database** using PyTorch tensors.  \n",
        "Each entry in the database will correspond to a text chunk and its embedding, enabling efficient retrieval based on vector similarity.\n"
      ],
      "metadata": {
        "id": "ctQ8gslMjKmR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 4: </b><br>\n",
        "Fill in the code to populate the database by computing embeddings for all text chunks in <code>chunks_df</code> using the <code>embed()</code> function.\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>\n"
      ],
      "metadata": {
        "id": "T1WHRh10VYG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def populate_database(chunks_df, batch_size=16):\n",
        "    \"\"\"\n",
        "    Populates a vector database from precomputed chunks_df.\n",
        "    \"\"\"\n",
        "    n_chunks = len(chunks_df)\n",
        "\n",
        "    sample_emb = embed([chunks_df[\"chunk_text\"].iloc[0]], doc_type=\"document\")  ## FILL THE GAP: compute one sample embedding to infer the output dimension\n",
        "    output_dim = sample_emb.shape[1] ## FILL THE GAP: extract the embedding dimension from the sample\n",
        "\n",
        "    vectorial_database = torch.zeros((n_chunks, output_dim)) ## FILL THE GAP: initialize an empty tensor to store all embeddings\n",
        "    chunk_list = chunks_df[\"chunk_text\"].tolist()\n",
        "\n",
        "    print(f\"Populating vector database with {n_chunks} chunks...\")\n",
        "\n",
        "    n = 0\n",
        "    for i in range(0, n_chunks, batch_size):\n",
        "        batch = chunk_list[i:i + batch_size] ## FILL THE GAP: select a batch of chunk texts\n",
        "        embeddings = embed(batch, doc_type=\"document\") ## FILL THE GAP: compute embeddings for the current batch\n",
        "        vectorial_database[n:n + len(batch)] = embeddings ## FILL THE GAP: store embeddings in the tensor\n",
        "        n += len(batch)\n",
        "\n",
        "    return chunk_list, vectorial_database"
      ],
      "metadata": {
        "id": "3Va4H572IQ67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build Vector Database\n",
        "chunk_list, vectorial_database = populate_database(chunks_df)\n",
        "\n",
        "print(\"\\n✅ Vector database successfully built.\")\n",
        "print(f\"Total stored chunks: {len(chunk_list)}\")\n",
        "print(f\"Database tensor shape: {tuple(vectorial_database.shape)}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "piSelCZ6IQ9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save vector databse\n",
        "os.makedirs(\"vector_db\", exist_ok=True)\n",
        "\n",
        "# Save tensor + chunk list\n",
        "torch.save(vectorial_database, \"vector_db/vectorial_database.pth\")\n",
        "\n",
        "with open(\"vector_db/chunk_list.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(chunk_list, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(\"✅ Saved:\")\n",
        "print(\" - vector_db/vectorial_database.pth\")\n",
        "print(\" - vector_db/chunk_list.json\")"
      ],
      "metadata": {
        "id": "dE7hrQ95JMVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the database\n",
        "vectorial_database = torch.load(\"vector_db/vectorial_database.pth\", map_location=device)\n",
        "vectorial_database.requires_grad_(False)\n",
        "\n",
        "with open(\"vector_db/chunk_list.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    chunk_list = json.load(f)\n",
        "\n",
        "print(f\"✅ Loaded {len(chunk_list)} chunks.\")\n",
        "print(f\"Database shape: {vectorial_database.shape}\\n\")\n",
        "\n",
        "# Inspect first few entries\n",
        "for i, embedding_vector in enumerate(vectorial_database[:5]):\n",
        "    print(f\"Vector {i} → {embedding_vector[:5]}\")\n",
        "    print(f\"Text snippet: {chunk_list[i][:300]}\\n\")\n"
      ],
      "metadata": {
        "id": "KfiCmzroIpu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Defining Similarity Metrics for Retrieval**\n",
        "\n",
        "After populating our vector database with embeddings, the next step in a RAG pipeline is to define a *similarity metric* to measure how close two vectors are in the embedding space.  \n",
        "Common metrics include **dot product**, **L2 distance**, and **cosine similarity**.  \n",
        "\n",
        "In most retrieval systems, cosine similarity is preferred because it measures the *angle* between two vectors rather than their magnitude, allowing comparison based purely on semantic direction instead of scale.\n"
      ],
      "metadata": {
        "id": "M_YHXmlylFbn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 5: </b><br>\n",
        "Fill in the code to implement the <code>cosine_similarity()</code> function.  \n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>\n"
      ],
      "metadata": {
        "id": "idmh4G__lGTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(query_embeddings, doc_embeddings):\n",
        "    \"\"\"\n",
        "    Computes cosine similarity between query and document embeddings\n",
        "    using manual normalization.\n",
        "    \"\"\"\n",
        "    query_magnitudes = torch.norm(query_embeddings, dim=1, keepdim=True) ## FILL THE GAP: calculate vector length for each query\n",
        "    normalized_queries = query_embeddings / query_magnitudes ## FILL THE GAP: normalize queries using their magnitudes\n",
        "\n",
        "    doc_magnitudes = torch.norm(doc_embeddings, dim=1, keepdim=True) ## FILL THE GAP: calculate vector length for each document\n",
        "    normalized_docs = doc_embeddings / doc_magnitudes ## FILL THE GAP: normalize documents using their magnitudes\n",
        "\n",
        "    similarity_matrix = torch.mm(normalized_queries, normalized_docs.transpose(0, 1)) ## FILL THE GAP: perform dot product between normalized queries and documents\n",
        "\n",
        "    return similarity_matrix\n",
        "\n",
        "\n",
        "# --- Example test ---\n",
        "query_embeddings = embed([\n",
        "    \"What is t-SNE?\",\n",
        "    \"Who is Laurens van der Maaten?\"\n",
        "], \"query\")\n",
        "\n",
        "doc_embeddings = embed([\n",
        "    \"t-SNE is a dimensionality reduction algorithm created by Laurens van der Maaten.\"\n",
        "], \"document\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    sim_cos = cosine_similarity(query_embeddings, doc_embeddings)\n",
        "\n",
        "print(\"🔍 Example cosine similarity scores:\\n\", sim_cos)"
      ],
      "metadata": {
        "id": "IRK6PNQhK1LB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 6: </b><br>\n",
        "Fill in the code to complete the <code>retrieve()</code> function.  \n",
        "It should encode the input query using <code>embed()</code>, compute similarity with all vectors in <code>vectorial_database</code>, and return the top-<i>k</i> most similar text chunks.\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>\n"
      ],
      "metadata": {
        "id": "PeIEzoebV8wd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve(query,\n",
        "             vectorial_database=vectorial_database,\n",
        "             chunk_list=chunk_list,\n",
        "             topk=5,\n",
        "             verbose=False):\n",
        "    \"\"\"\n",
        "    Retrieves top-k most similar chunks to a query using dot-product similarity.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        query_embedding = embed([query], doc_type=\"query\")[0]  ## FILL THE GAP: encode the input query using the embed() function\n",
        "        query_embedding = query_embedding.to(device) ## FILL THE GAP: move the query embedding to the correct device\n",
        "        similarity_scores = torch.mm(\n",
        "            query_embedding.unsqueeze(0),\n",
        "            vectorial_database.t()\n",
        "        ) ## FILL THE GAP: compute similarity between query and database embeddings\n",
        "        topk_results = torch.topk(similarity_scores, k=topk) ## FILL THE GAP: extract the top-k highest similarity scores and indices\n",
        "\n",
        "        if verbose:\n",
        "            for score, idx in zip(topk_results.values[0], topk_results.indices[0]):\n",
        "                print(f\"\\nScore: {score:.4f}\")\n",
        "                print(f\"Chunk:\\n{chunk_list[idx][:500]}\\n{'-'*80}\")\n",
        "\n",
        "        retrieved_chunks = [chunk_list[idx] for idx in topk_results.indices[0]]   ## FILL THE GAP: select text chunks corresponding to the top-k indices\n",
        "        return \"\\n\\n\".join(retrieved_chunks) ## FILL THE GAP: return concatenated retrieved chunks as a single string"
      ],
      "metadata": {
        "id": "y4Wv_MD1K6IL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example query\n",
        "query = \"When was Luigi Boria born?\" #Try different queries based on the documents in the wikipedia dataset\n",
        "result = retrieve(query, topk=3, verbose=True)\n"
      ],
      "metadata": {
        "id": "HsqTAtYAK6LS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='red'>\n",
        "<hr style=\"border:10px solid red\"> </hr> Question 4: There are retrieval methods like BM25 that rely on lexical overlap between the query and documents, and others based on dense embeddings that capture semantic similarity beyond exact word matches. Explain how these two approaches differ in how they represent and compare text. Then, discuss how a hybrid retrieval strategy combining both can overcome their respective limitations and improve retrieval performance in RAG systems. <hr style=\"border:10px solid red\"> </hr> </font></h4>"
      ],
      "metadata": {
        "id": "u8AR7jFOLcPA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 4: </b><br>\n",
        "Your answer here.\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>\n",
        "\n",
        "\n",
        "**BM25** uses sparse vectors based on exact word matching and term frequency statistics. It counts how often query words appear in documents, weighted by their rarity across the corpus. However, it fails to recognize synonyms \"car\" and \"automobile\" are treated as completely different terms. It cannot handle paraphrasing or understand semantic meaning beyond surface level word overlap.\n",
        "\n",
        "**Dense embeddings** map text into continuous vector spaces using neural networks. They capture conceptual relationships and contextual meaning regardless of exact words used. They understand that \"vehicle\" and \"car\" are semantically related. But they may miss important exact keyword matches and struggle with rare technical terms not seen during training.\n",
        "\n",
        "**Hybrid retrieval** combines both approaches for better performance. It retrieves candidates using both BM25 and dense embeddings separately. Results are merged using techniques like Reciprocal Rank Fusion, which combines rankings from both methods. This captures BM25's strength with specific keywords, proper nouns, and technical jargon.\n",
        "\n",
        "**For RAG systems**, hybrid retrieval is particularly valuable because users ask diverse question types. Some queries need exact term matching. Others need semantic understanding. Hybrid approaches achieve 10-30% better recall and precision compared to using either method alone. They make RAG systems more robust and reliable across different query styles and document types."
      ],
      "metadata": {
        "id": "tTT638v7MeMn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In real-world RAG systems, instead of manually storing and comparing vectors, we rely on **vector databases** such as **ChromaDB**, which are optimized for efficient **similarity search**, **indexing**, and **retrieval** at scale.  \n",
        "\n",
        "These databases provide:\n",
        "- Fast nearest-neighbor search (e.g., using HNSW graphs)  \n",
        "- Persistent storage for millions of embeddings  \n",
        "- Built-in support for different similarity metrics (cosine, L2, inner product)  \n"
      ],
      "metadata": {
        "id": "Rpq8WQilG8LE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "\n",
        "# --- Initialize ChromaDB client ---\n",
        "chroma_client = chromadb.Client(Settings(\n",
        "    anonymized_telemetry=False,\n",
        "    allow_reset=True\n",
        "))\n",
        "\n",
        "# Reset ensures a clean state\n",
        "chroma_client.reset()\n",
        "\n",
        "# --- Create a collection ---\n",
        "# You can choose the similarity metric: \"cosine\", \"l2\", or \"ip\" (inner product)\n",
        "collection_name = \"wikipedia_chunks\"\n",
        "collection = chroma_client.create_collection(\n",
        "    name=collection_name,\n",
        "    metadata={\"hnsw:space\": \"cosine\"}  # cosine similarity works best for normalized embeddings\n",
        ")\n",
        "\n",
        "print(f\"✅ Created collection: {collection_name}\")\n"
      ],
      "metadata": {
        "id": "digvIbGF8h6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the data\n",
        "ids = chunks_df[\"chunk_id\"].tolist()\n",
        "embeddings = chunks_df[\"embedding\"].tolist()\n",
        "documents = chunks_df[\"chunk_text\"].tolist()\n",
        "\n",
        "# Ensure all embeddings are plain Python lists\n",
        "embeddings = [e.tolist() if hasattr(e, \"tolist\") else e for e in embeddings]\n",
        "\n",
        "# Add useful metadata for inspection\n",
        "metadatas = [\n",
        "    {\n",
        "        \"doc_id\": row[\"doc_id\"],\n",
        "        \"title\": row[\"title\"],\n",
        "        \"url\": row[\"url\"]\n",
        "    }\n",
        "    for _, row in chunks_df.iterrows()\n",
        "]\n",
        "\n",
        "# Add to the collection\n",
        "collection.add(\n",
        "    ids=ids,\n",
        "    embeddings=embeddings,\n",
        "    documents=documents,\n",
        "    metadatas=metadatas\n",
        ")\n",
        "\n",
        "print(f\"\\n✅ Added {collection.count()} chunks to the collection\")\n",
        "print(f\"Collection metadata: {collection.metadata}\")\n"
      ],
      "metadata": {
        "id": "c6y61_JSNA_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the query using our embed() function\n",
        "query = \"When was Luigi Boria born?\"\n",
        "query_embedding = embed([query], doc_type=\"query\")[0].tolist()  # get single vector as list\n",
        "\n",
        "# Query the collection\n",
        "results = collection.query(\n",
        "    query_embeddings=[query_embedding],\n",
        "    n_results=3 # number of retrieved results\n",
        ")\n",
        "\n",
        "# Display results\n",
        "print(f\"🔎 Query: {query}\\n\" + \"=\" * 80)\n",
        "for i in range(len(results[\"documents\"][0])):\n",
        "    print(f\"\\nResult {i+1}:\")\n",
        "    print(f\"Title: {results['metadatas'][0][i]['title']}\")\n",
        "    print(f\"Similarity score: {1 - results['distances'][0][i]:.4f}\")  # cosine distance → similarity\n",
        "    print(f\"Text: {results['documents'][0][i][:300]}...\")\n",
        "    print(\"-\" * 80)\n"
      ],
      "metadata": {
        "id": "CHysGgWTNIKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='red'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Question 5: </b><br>\n",
        "ChromaDB and other vector databases often rely on Hierarchical Navigable Small World (HNSW) graphs to perform efficient approximate nearest neighbor search.  \n",
        "Explain how the HNSW algorithm organizes data to enable fast and accurate retrieval in high-dimensional spaces.  \n",
        "Why is this structure particularly effective for large-scale embedding collections compared to brute-force search?\n",
        "\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "<i>Reference:</i> <a href=\"https://arxiv.org/pdf/1603.09320\" target=\"_blank\">Efficient and Robust Approximate Nearest Neighbor Search using Hierarchical Navigable Small World Graphs</a>\n",
        "</font></h4>\n"
      ],
      "metadata": {
        "id": "n2GomUuzYFdR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 5: </b><br>\n",
        "Your answer here.\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>\n",
        "\n",
        "**HNSW** is a graph-based indexing structure designed for fast approximate nearest neighbor search in high-dimensional spaces. It builds on the concept of small-world networks where most nodes are reachable through a small number of hops.\n",
        "\n",
        "**Core Structure**: HNSW organizes embeddings into multiple hierarchical layers. The top layers are sparse and contain long-range connections for rapid navigation. Bottom layers are dense with short-range connections for precise local search. Each data point appears in layer 0, but only some points propagate to higher layers with exponentially decreasing probability.\n",
        "\n",
        "**Search Process**: Search starts at the topmost layer with a random entry point. It greedily navigates toward the query using long-range connections to quickly approach the target region. As it descends through layers, it refines the search with increasingly local connections. At layer 0, it performs a final greedy search among nearest candidates. This multi-scale approach mimics zooming from a global map view to street-level detail.\n",
        "\n",
        "**Why It's Effective**: HNSW achieves search complexity O(log N) instead of linear O(N) in brute-force search. For a collection of 1 million embeddings, brute-force requires 1 million distance calculations. HNSW needs only hundreds. The hierarchical structure enables efficient pruning. Long-range connections in upper layers allow the algorithm to skip large irrelevant regions.\n",
        "\n",
        "**For Vector Databases**: HNSW provides very recall value (>95%) while being 100-1000× faster than exhaustive search. It handles billion-scale collections that would be impractical with brute-force. Memory efficiency comes from storing only graph connections, not redundant data copies. This makes HNSW effective for RAG systems where embedding collections grow continuously and query latency must remain low."
      ],
      "metadata": {
        "id": "G5hHNO3LYMSa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <b> Two-Stage Retrieval: Dense Retrieval + Reranker</b>\n",
        "\n",
        "In RAG, the first retrieval step often returns passages that are similar in meaning but not always the most relevant.  \n",
        "A **reranker** fixes this by re-evaluating the top retrieved chunks using a stronger model that jointly reads the query and each document to assign a more accurate relevance score.\n"
      ],
      "metadata": {
        "id": "CR9LpejHea5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "# Load reranker model\n",
        "RERANKER_MODEL_NAME = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
        "reranker = CrossEncoder(RERANKER_MODEL_NAME)\n",
        "\n",
        "def retrieve_with_reranker(query, collection, initial_k=5, final_k=3):\n",
        "    \"\"\"\n",
        "    Retrieves and reranks candidate documents for a given query.\n",
        "    Returns both the initial dense results and reranked results.\n",
        "    \"\"\"\n",
        "    query_embedding = embed([query], doc_type=\"query\")[0].tolist()\n",
        "    candidates = collection.query(query_embeddings=[query_embedding], n_results=initial_k)\n",
        "\n",
        "    docs = candidates[\"documents\"][0]\n",
        "    metas = candidates[\"metadatas\"][0]\n",
        "    dense_scores = [(1 - s) for s in candidates[\"distances\"][0]]\n",
        "\n",
        "    pairs = [(query, d) for d in docs]\n",
        "    ce_scores = reranker.predict(pairs)\n",
        "\n",
        "    reranked = [\n",
        "        {\n",
        "            \"title\": metas[i].get(\"title\", \"\"),\n",
        "            \"url\": metas[i].get(\"url\", \"\"),\n",
        "            \"text\": docs[i],\n",
        "            \"dense_score\": dense_scores[i],\n",
        "            \"rerank_score\": float(ce_scores[i]),\n",
        "        }\n",
        "        for i in range(len(docs))\n",
        "    ]\n",
        "    reranked.sort(key=lambda x: x[\"rerank_score\"], reverse=True)\n",
        "\n",
        "    return docs, metas, dense_scores, reranked[:final_k]"
      ],
      "metadata": {
        "id": "urSlPBdORkab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "query = \"Where did Luigi Boria study?\" #Try other queries too\n",
        "docs, metas, dense_scores, top_reranked = retrieve_with_reranker(\n",
        "    query=query,\n",
        "    collection=collection,\n",
        "    initial_k=5,\n",
        "    final_k=3\n",
        ")\n",
        "\n",
        "# --- Display initial dense retrieval ---\n",
        "print(f\"\\nInitial dense retrieval (Top 5):\\n\" + \"=\" * 80)\n",
        "for i, (d, s, m) in enumerate(zip(docs, dense_scores, metas), 1):\n",
        "    print(f\"{i}. {m.get('title', '')}  |  Dense similarity: {s:.4f}\")\n",
        "    print(f\"Text: {d[:300].replace('\\n', ' ')}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "# --- Display top reranked results ---\n",
        "print(f\"\\nAfter Cross-Encoder Reranking (Top 3):\\n\" + \"=\" * 80)\n",
        "for i, item in enumerate(top_reranked, 1):\n",
        "    print(f\"{i}. {item['title']}\")\n",
        "    print(f\"Dense similarity: {item['dense_score']:.4f} | Reranker score: {item['rerank_score']:.4f}\")\n",
        "    print(f\"Text: {item['text'][:300].replace('\\n', ' ')}\")\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "id": "A9eao4mXtXkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Bi-Encoder vs Cross-Encoder Architecture](https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/docs/img/Bi_vs_Cross-Encoder.png)\n",
        "\n",
        "<b><h4><font color='red'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Question 6:  \n",
        "The figure above compares a Bi-Encoder and a Cross-Encoder architecture.  \n",
        "Rerankers such as <code>cross-encoder/ms-marco-MiniLM-L-6-v2</code> use the second approach, jointly encoding the query and document through a single transformer.  \n",
        "Why does this joint encoding typically yield higher retrieval precision, and why is it applied as a second-stage reranker instead of being used directly for large-scale retrieval?\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "</font></h4>\n"
      ],
      "metadata": {
        "id": "w_sPNBkW1tNL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 6: </b><br>\n",
        "Your answer here.\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>\n",
        "\n",
        "### Why Cross-Encoders Have Higher Precision\n",
        "\n",
        "**Joint encoding allows richer interaction:**\n",
        "- The Cross-Encoder processes query and document together through all transformer layers\n",
        "- Attention mechanisms can compute token-level interactions between query and document from the start\n",
        "- The model learns nuanced semantic relationships: which document tokens specifically answer which query tokens\n",
        "- Results in much more accurate relevance scoring\n",
        "\n",
        "**Bi-Encoder limitations:**\n",
        "- Encodes query and document independently into separate embeddings\n",
        "- Only compares them via simple cosine similarity at the end\n",
        "- No direct attention between query and document tokens\n",
        "- Misses fine-grained semantic matching\n",
        "\n",
        "### Why Cross-Encoders Are Only Used for Reranking and not directly for large-scale retrieval\n",
        "\n",
        "**Computational cost:**\n",
        "- Cross-Encoder must process every (query, document) pair jointly\n",
        "- For 1 query + 1 million documents = 1 million forward passes\n",
        "- Does not scale to large document collections\n",
        "\n",
        "**Bi-Encoder advantage for first-stage retrieval:**\n",
        "- Encode all documents once into embeddings, store them\n",
        "- At query time: encode query once, compute cosine similarity with pre-computed document embeddings\n",
        "- Fast approximate nearest neighbor search (ANN) retrieves top-k candidates in milliseconds\n",
        "\n",
        "## The Two-Stage Pipeline\n",
        "\n",
        "1. **Bi-Encoder (retrieval)**: Fast, scalable - retrieve top 100-1000 candidates from millions\n",
        "2. **Cross-Encoder (reranking)**: Slow, accurate - precisely rerank the small candidate set\n"
      ],
      "metadata": {
        "id": "-xxephXI16n9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Integrating Retrieved Context into the LLM’s Prompt**\n",
        "\n",
        "Now that we can retrieve and rerank the most relevant document chunks,  \n",
        "we integrate them directly into the **language model’s prompt**.  \n",
        "This step allows the model to **ground its answer on factual context** rather than relying solely on internal knowledge —  \n",
        "thereby improving accuracy and reducing hallucinations.\n"
      ],
      "metadata": {
        "id": "fTDWX3E82gkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "gen_model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "gen_tok = AutoTokenizer.from_pretrained(gen_model_name, trust_remote_code=True)\n",
        "gen_model = AutoModelForCausalLM.from_pretrained(\n",
        "    gen_model_name,\n",
        "    device_map=\"auto\",\n",
        "    dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "def generate(msg, max_new_tokens=128, temperature=0.2):\n",
        "    messages = [{\"role\": \"user\", \"content\": msg}]\n",
        "    inputs = gen_tok.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(gen_model.device)\n",
        "\n",
        "    outputs = gen_model.generate(\n",
        "        inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=temperature,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        eos_token_id=gen_tok.eos_token_id,\n",
        "        pad_token_id=gen_tok.pad_token_id if gen_tok.pad_token_id is not None else gen_tok.eos_token_id\n",
        "    )\n",
        "\n",
        "    return gen_tok.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True).strip()\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# NO-RAG vs WITH RAG\n",
        "# ============================================================\n",
        "query = \"When was Luigi Boria born?\" #Try other queries\n",
        "\n",
        "print(\"ORIGINAL PROMPT\\n\" + \"=\" * 60)\n",
        "print(query)\n",
        "\n",
        "print(\"\\nANSWER WITHOUT RAG\\n\" + \"=\" * 60)\n",
        "print(generate(query))\n",
        "\n",
        "docs, metas, dense_scores, top_reranked = retrieve_with_reranker(\n",
        "    query=query,\n",
        "    collection=collection,\n",
        "    initial_k=5,\n",
        "    final_k=3\n",
        ")\n",
        "\n",
        "context = \"\\n\\n\".join(h[\"text\"] for h in top_reranked)[:1600]\n",
        "rag_prompt = (\n",
        "    f\"Use only the context to answer. If unknown, say you don't know.\\n\\n\"\n",
        "    f\"Context:\\n{context}\\n\\n\"\n",
        "    f\"Question: {query}\\nAnswer:\"\n",
        ")\n",
        "\n",
        "print(\"\\nAUGMENTED PROMPT\\n\" + \"=\" * 60)\n",
        "print(rag_prompt)\n",
        "\n",
        "print(\"\\nANSWER WITH RAG\\n\" + \"=\" * 60)\n",
        "print(generate(rag_prompt))\n"
      ],
      "metadata": {
        "id": "qqwTZr0vJZw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='red'>\n",
        "<hr style=\"border:10px solid red\"> </hr> Question 7: In the RAG prompt construction step, we simply concatenate the retrieved chunks before the question. Discuss potential issues with this naive approach, such as token limits, redundancy, or irrelevant context dilution. Then, explain how we could select, weight, or summarize the retrieved chunks before injecting them into the prompt to improve generation quality and efficiency. <hr style=\"border:10px solid red\"> </hr> </font></h4>"
      ],
      "metadata": {
        "id": "b0Rlc7lVNBoj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "Answer 7: </b><br>\n",
        "Your answer here.\n",
        "<hr style=\"border:10px solid green\"> </hr>\n",
        "</font></h4>\n",
        "\n",
        "**Problems with Naive Concatenation:**\n",
        "\n",
        "- Token limits: Long concatenated chunks exhaust context windows and increase costs.\n",
        "- Redundancy: Multiple chunks repeat the same information, wasting tokens.\n",
        "- Noise: Irrelevant chunks dilute important context and confuse the model (\"lost in the middle\" effect).\n",
        "\n",
        "**Better Approaches:**\n",
        "- Reranking: Use cross-encoders to score chunks and select only top 3-5 most relevant ones.\n",
        "- Compression: Summarize chunks extractively or abstractively to reduce length while preserving key information.\n",
        "- Weighting: Position most relevant chunks near the question where models attend better. Add relevance scores as metadata."
      ],
      "metadata": {
        "id": "TX3eiuf6NIbV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **To go further**\n",
        "\n",
        "- Experiment with other chunking methods (e.g., semantic or recursive chunking).  \n",
        "- Explore **LangChain** and **LlamaIndex** for building modular RAG pipelines.  \n",
        "- Try **hybrid retrieval** combining sparse (BM25) and dense embeddings.  \n",
        "- Explore more advanced RAG methods such as **RAG-Fusion**, **Self-RAG**, and **Active-RAG**.  \n",
        "- Experiment with **Matryoshka Representation Learning** for scalable embeddings.  \n",
        "- Try **fine-tuning rerankers** or **retrievers** for domain-specific data.  \n"
      ],
      "metadata": {
        "id": "2dqvyNj-oSe7"
      }
    }
  ]
}